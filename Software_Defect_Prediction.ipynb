{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Software-Defect-Prediction",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBosPwVESjHC3I8dpKDv32"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEMlVb_5biF0"
      },
      "source": [
        "Importing all nescessary packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWiqWojS2qJW"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import seaborn as sns\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdCeRHRAcFkb"
      },
      "source": [
        "Downloading & Printing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pSiCustcEzK"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/MilanBinsMathew/Software-Defect-Prediction-JM1/main/jm1_csv.csv'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFKX_vJCd1Bk",
        "outputId": "603c00a5-2bc5-462e-85a9-50cc01ba0f21"
      },
      "source": [
        "df = pd.read_csv(url)\n",
        "print(df.head())\n",
        "\n",
        "df.isnull().sum()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     loc  v(g)  ev(g)  iv(g)  ...  total_Op  total_Opnd  branchCount  defects\n",
            "0    1.1   1.4    1.4    1.4  ...       1.2         1.2          1.4        0\n",
            "1    1.0   1.0    1.0    1.0  ...       1.0         1.0          1.0        1\n",
            "2   72.0   7.0    1.0    6.0  ...     112.0        86.0         13.0        1\n",
            "3  190.0   3.0    1.0    3.0  ...     329.0       271.0          5.0        1\n",
            "4   37.0   4.0    1.0    4.0  ...      76.0        50.0          7.0        1\n",
            "\n",
            "[5 rows x 22 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "loc                  0\n",
              "v(g)                 0\n",
              "ev(g)                0\n",
              "iv(g)                0\n",
              "n                    0\n",
              "v                    0\n",
              "l                    0\n",
              "d                    0\n",
              "i                    0\n",
              "e                    0\n",
              "b                    0\n",
              "t                    0\n",
              "lOCode               0\n",
              "lOComment            0\n",
              "lOBlank              0\n",
              "locCodeAndComment    0\n",
              "uniq_Op              0\n",
              "uniq_Opnd            0\n",
              "total_Op             0\n",
              "total_Opnd           0\n",
              "branchCount          0\n",
              "defects              0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2m3WgW3l7zp"
      },
      "source": [
        "Shuffling the dataset.\n",
        "Segementation into training and test set (80% - 20%)\n",
        "Normalizing the values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSG4zkugmM0Y"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop(['defects'], axis =1).values\n",
        "\n",
        "y = df['defects'].values\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3VMk2pJri8R"
      },
      "source": [
        "Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G-KTc-ArmWC"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=[0,1])\n",
        "X = scaler.fit_transform(X) \n",
        "\n",
        "train_X,test_X,train_Y,test_Y = train_test_split(X, y, test_size=0.2, random_state = 2)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2rWS3crnMva"
      },
      "source": [
        "\n",
        "\n",
        "Keras Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbso4gW0r_zM"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Activation,BatchNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TZgUlkbxblE"
      },
      "source": [
        "\n",
        "\n",
        "model = Sequential([ \n",
        "(Dense(32,activation = 'relu',input_shape = (21,))),\n",
        "(BatchNormalization()),\n",
        "(Dense(32,activation = 'relu')),\n",
        "(BatchNormalization()),\n",
        "(Dense(64,activation = 'relu')),\n",
        "(Dense(64,activation = 'relu')),\n",
        "(Dense(64,activation = 'relu')),\n",
        "(Dense(64,activation = 'relu')),\n",
        "(BatchNormalization()),\n",
        "(Dense(32,activation = 'relu')),\n",
        "(Dense(32,activation = 'relu')),\n",
        "(Dense(1,activation = 'linear'))\n",
        "])\n",
        "ad = Adam(learning_rate = 0.00001)\n",
        "\n",
        "\n",
        "model.compile(optimizer = ad, loss='hinge',metrics = ['accuracy'])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGMY88g8rQuO"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88pIatPU85m-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6d73cf-aaa9-46dc-ad02-6e18c3aebf14"
      },
      "source": [
        "model.fit(x = train_X, y = train_Y, batch_size = 64, epochs = 250)\n",
        "model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "136/136 [==============================] - 2s 4ms/step - loss: 1.0615 - accuracy: 0.7841\n",
            "Epoch 2/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.9489 - accuracy: 0.7979\n",
            "Epoch 3/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.8658 - accuracy: 0.8043\n",
            "Epoch 4/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.8131 - accuracy: 0.8056\n",
            "Epoch 5/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.7556 - accuracy: 0.8081\n",
            "Epoch 6/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.6997 - accuracy: 0.8089\n",
            "Epoch 7/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.6471 - accuracy: 0.8082\n",
            "Epoch 8/250\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.5950 - accuracy: 0.8100\n",
            "Epoch 9/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.5566 - accuracy: 0.8096\n",
            "Epoch 10/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.5228 - accuracy: 0.8102\n",
            "Epoch 11/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4987 - accuracy: 0.8095\n",
            "Epoch 12/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4713 - accuracy: 0.8095\n",
            "Epoch 13/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4625 - accuracy: 0.8086\n",
            "Epoch 14/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4460 - accuracy: 0.8107\n",
            "Epoch 15/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4417 - accuracy: 0.8100\n",
            "Epoch 16/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4324 - accuracy: 0.8109\n",
            "Epoch 17/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.4288 - accuracy: 0.8108\n",
            "Epoch 18/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4251 - accuracy: 0.8108\n",
            "Epoch 19/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4229 - accuracy: 0.8103\n",
            "Epoch 20/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4225 - accuracy: 0.8107\n",
            "Epoch 21/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4176 - accuracy: 0.8103\n",
            "Epoch 22/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4169 - accuracy: 0.8097\n",
            "Epoch 23/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4152 - accuracy: 0.8115\n",
            "Epoch 24/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.4143 - accuracy: 0.8115\n",
            "Epoch 25/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4115 - accuracy: 0.8123\n",
            "Epoch 26/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4116 - accuracy: 0.8116\n",
            "Epoch 27/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4141 - accuracy: 0.8116\n",
            "Epoch 28/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.4082 - accuracy: 0.8119\n",
            "Epoch 29/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.4111 - accuracy: 0.8102\n",
            "Epoch 30/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4060 - accuracy: 0.8132\n",
            "Epoch 31/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4091 - accuracy: 0.8122\n",
            "Epoch 32/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4038 - accuracy: 0.8126\n",
            "Epoch 33/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4035 - accuracy: 0.8125\n",
            "Epoch 34/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4028 - accuracy: 0.8120\n",
            "Epoch 35/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4030 - accuracy: 0.8133\n",
            "Epoch 36/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.3989 - accuracy: 0.8135\n",
            "Epoch 37/250\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3970 - accuracy: 0.8139\n",
            "Epoch 38/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.4020 - accuracy: 0.8131\n",
            "Epoch 39/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.4009 - accuracy: 0.8139\n",
            "Epoch 40/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3991 - accuracy: 0.8132\n",
            "Epoch 41/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4007 - accuracy: 0.8135\n",
            "Epoch 42/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3995 - accuracy: 0.8131\n",
            "Epoch 43/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4012 - accuracy: 0.8125\n",
            "Epoch 44/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3962 - accuracy: 0.8146\n",
            "Epoch 45/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3981 - accuracy: 0.8146\n",
            "Epoch 46/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3961 - accuracy: 0.8154\n",
            "Epoch 47/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.3947 - accuracy: 0.8133\n",
            "Epoch 48/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.3943 - accuracy: 0.8142\n",
            "Epoch 49/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3936 - accuracy: 0.8155\n",
            "Epoch 50/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3934 - accuracy: 0.8150\n",
            "Epoch 51/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3938 - accuracy: 0.8156\n",
            "Epoch 52/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3921 - accuracy: 0.8142\n",
            "Epoch 53/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3926 - accuracy: 0.8146\n",
            "Epoch 54/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3951 - accuracy: 0.8139\n",
            "Epoch 55/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.3919 - accuracy: 0.8155\n",
            "Epoch 56/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3933 - accuracy: 0.8145\n",
            "Epoch 57/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3934 - accuracy: 0.8139\n",
            "Epoch 58/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3928 - accuracy: 0.8142\n",
            "Epoch 59/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3903 - accuracy: 0.8141\n",
            "Epoch 60/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3901 - accuracy: 0.8156\n",
            "Epoch 61/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3935 - accuracy: 0.8143\n",
            "Epoch 62/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3904 - accuracy: 0.8161\n",
            "Epoch 63/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3903 - accuracy: 0.8151\n",
            "Epoch 64/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3890 - accuracy: 0.8147\n",
            "Epoch 65/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3917 - accuracy: 0.8147\n",
            "Epoch 66/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3907 - accuracy: 0.8154\n",
            "Epoch 67/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3864 - accuracy: 0.8168\n",
            "Epoch 68/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3898 - accuracy: 0.8165\n",
            "Epoch 69/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3894 - accuracy: 0.8148\n",
            "Epoch 70/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3916 - accuracy: 0.8138\n",
            "Epoch 71/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3904 - accuracy: 0.8146\n",
            "Epoch 72/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3884 - accuracy: 0.8151\n",
            "Epoch 73/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.3884 - accuracy: 0.8161\n",
            "Epoch 74/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3877 - accuracy: 0.8153\n",
            "Epoch 75/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3890 - accuracy: 0.8153\n",
            "Epoch 76/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3852 - accuracy: 0.8165\n",
            "Epoch 77/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3893 - accuracy: 0.8153\n",
            "Epoch 78/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3867 - accuracy: 0.8153\n",
            "Epoch 79/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3887 - accuracy: 0.8156\n",
            "Epoch 80/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3860 - accuracy: 0.8162\n",
            "Epoch 81/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3854 - accuracy: 0.8173\n",
            "Epoch 82/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.3866 - accuracy: 0.8156\n",
            "Epoch 83/250\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3823 - accuracy: 0.8163\n",
            "Epoch 84/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3872 - accuracy: 0.8154\n",
            "Epoch 85/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3857 - accuracy: 0.8158\n",
            "Epoch 86/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3865 - accuracy: 0.8154\n",
            "Epoch 87/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3854 - accuracy: 0.8157\n",
            "Epoch 88/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3863 - accuracy: 0.8155\n",
            "Epoch 89/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3854 - accuracy: 0.8163\n",
            "Epoch 90/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3842 - accuracy: 0.8165\n",
            "Epoch 91/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.3856 - accuracy: 0.8158\n",
            "Epoch 92/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3837 - accuracy: 0.8157\n",
            "Epoch 93/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3828 - accuracy: 0.8162\n",
            "Epoch 94/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3815 - accuracy: 0.8165\n",
            "Epoch 95/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3836 - accuracy: 0.8172\n",
            "Epoch 96/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3820 - accuracy: 0.8169\n",
            "Epoch 97/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3842 - accuracy: 0.8153\n",
            "Epoch 98/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3847 - accuracy: 0.8157\n",
            "Epoch 99/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3825 - accuracy: 0.8173\n",
            "Epoch 100/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3834 - accuracy: 0.8168\n",
            "Epoch 101/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3821 - accuracy: 0.8164\n",
            "Epoch 102/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3810 - accuracy: 0.8168\n",
            "Epoch 103/250\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3801 - accuracy: 0.8184\n",
            "Epoch 104/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3809 - accuracy: 0.8180\n",
            "Epoch 105/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3840 - accuracy: 0.8158\n",
            "Epoch 106/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3814 - accuracy: 0.8180\n",
            "Epoch 107/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3811 - accuracy: 0.8165\n",
            "Epoch 108/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3815 - accuracy: 0.8165\n",
            "Epoch 109/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3828 - accuracy: 0.8154\n",
            "Epoch 110/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3825 - accuracy: 0.8163\n",
            "Epoch 111/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3818 - accuracy: 0.8151\n",
            "Epoch 112/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3803 - accuracy: 0.8166\n",
            "Epoch 113/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3796 - accuracy: 0.8169\n",
            "Epoch 114/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3823 - accuracy: 0.8168\n",
            "Epoch 115/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3808 - accuracy: 0.8154\n",
            "Epoch 116/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.3806 - accuracy: 0.8161\n",
            "Epoch 117/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3794 - accuracy: 0.8179\n",
            "Epoch 118/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3801 - accuracy: 0.8166\n",
            "Epoch 119/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3804 - accuracy: 0.8163\n",
            "Epoch 120/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3798 - accuracy: 0.8164\n",
            "Epoch 121/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3806 - accuracy: 0.8169\n",
            "Epoch 122/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3786 - accuracy: 0.8168\n",
            "Epoch 123/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3811 - accuracy: 0.8174\n",
            "Epoch 124/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3780 - accuracy: 0.8172\n",
            "Epoch 125/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3772 - accuracy: 0.8159\n",
            "Epoch 126/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3787 - accuracy: 0.8177\n",
            "Epoch 127/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3776 - accuracy: 0.8178\n",
            "Epoch 128/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3769 - accuracy: 0.8187\n",
            "Epoch 129/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3816 - accuracy: 0.8164\n",
            "Epoch 130/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3779 - accuracy: 0.8181\n",
            "Epoch 131/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3800 - accuracy: 0.8170\n",
            "Epoch 132/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3787 - accuracy: 0.8178\n",
            "Epoch 133/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3769 - accuracy: 0.8180\n",
            "Epoch 134/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3797 - accuracy: 0.8179\n",
            "Epoch 135/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3761 - accuracy: 0.8177\n",
            "Epoch 136/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3787 - accuracy: 0.8169\n",
            "Epoch 137/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3780 - accuracy: 0.8171\n",
            "Epoch 138/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3797 - accuracy: 0.8168\n",
            "Epoch 139/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3778 - accuracy: 0.8177\n",
            "Epoch 140/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3781 - accuracy: 0.8170\n",
            "Epoch 141/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3780 - accuracy: 0.8186\n",
            "Epoch 142/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3788 - accuracy: 0.8181\n",
            "Epoch 143/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3761 - accuracy: 0.8173\n",
            "Epoch 144/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3757 - accuracy: 0.8174\n",
            "Epoch 145/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3771 - accuracy: 0.8166\n",
            "Epoch 146/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3764 - accuracy: 0.8195\n",
            "Epoch 147/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3773 - accuracy: 0.8176\n",
            "Epoch 148/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3763 - accuracy: 0.8156\n",
            "Epoch 149/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3766 - accuracy: 0.8170\n",
            "Epoch 150/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3758 - accuracy: 0.8188\n",
            "Epoch 151/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3763 - accuracy: 0.8173\n",
            "Epoch 152/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3727 - accuracy: 0.8186\n",
            "Epoch 153/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3763 - accuracy: 0.8180\n",
            "Epoch 154/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3755 - accuracy: 0.8189\n",
            "Epoch 155/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3738 - accuracy: 0.8189\n",
            "Epoch 156/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3750 - accuracy: 0.8182\n",
            "Epoch 157/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3702 - accuracy: 0.8205\n",
            "Epoch 158/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3723 - accuracy: 0.8195\n",
            "Epoch 159/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3752 - accuracy: 0.8174\n",
            "Epoch 160/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3734 - accuracy: 0.8192\n",
            "Epoch 161/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3740 - accuracy: 0.8199\n",
            "Epoch 162/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3742 - accuracy: 0.8193\n",
            "Epoch 163/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3755 - accuracy: 0.8180\n",
            "Epoch 164/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3769 - accuracy: 0.8180\n",
            "Epoch 165/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3768 - accuracy: 0.8188\n",
            "Epoch 166/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3732 - accuracy: 0.8195\n",
            "Epoch 167/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3733 - accuracy: 0.8194\n",
            "Epoch 168/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3736 - accuracy: 0.8186\n",
            "Epoch 169/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.3743 - accuracy: 0.8193\n",
            "Epoch 170/250\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3748 - accuracy: 0.8188\n",
            "Epoch 171/250\n",
            "136/136 [==============================] - 0s 3ms/step - loss: 0.3747 - accuracy: 0.8197\n",
            "Epoch 172/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.3747 - accuracy: 0.8181\n",
            "Epoch 173/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3739 - accuracy: 0.8192\n",
            "Epoch 174/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3722 - accuracy: 0.8172\n",
            "Epoch 175/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3736 - accuracy: 0.8188\n",
            "Epoch 176/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3721 - accuracy: 0.8196\n",
            "Epoch 177/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3718 - accuracy: 0.8196\n",
            "Epoch 178/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3739 - accuracy: 0.8197\n",
            "Epoch 179/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3699 - accuracy: 0.8204\n",
            "Epoch 180/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3744 - accuracy: 0.8193\n",
            "Epoch 181/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3711 - accuracy: 0.8204\n",
            "Epoch 182/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3749 - accuracy: 0.8185\n",
            "Epoch 183/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3738 - accuracy: 0.8185\n",
            "Epoch 184/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3720 - accuracy: 0.8177\n",
            "Epoch 185/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3753 - accuracy: 0.8188\n",
            "Epoch 186/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.3725 - accuracy: 0.8192\n",
            "Epoch 187/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3725 - accuracy: 0.8188\n",
            "Epoch 188/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3705 - accuracy: 0.8197\n",
            "Epoch 189/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3728 - accuracy: 0.8186\n",
            "Epoch 190/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.3721 - accuracy: 0.8193\n",
            "Epoch 191/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3713 - accuracy: 0.8182\n",
            "Epoch 192/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.3725 - accuracy: 0.8197\n",
            "Epoch 193/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3733 - accuracy: 0.8185\n",
            "Epoch 194/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3731 - accuracy: 0.8195\n",
            "Epoch 195/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3729 - accuracy: 0.8186\n",
            "Epoch 196/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3690 - accuracy: 0.8202\n",
            "Epoch 197/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3706 - accuracy: 0.8199\n",
            "Epoch 198/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3728 - accuracy: 0.8192\n",
            "Epoch 199/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3725 - accuracy: 0.8201\n",
            "Epoch 200/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3704 - accuracy: 0.8194\n",
            "Epoch 201/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3718 - accuracy: 0.8185\n",
            "Epoch 202/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3713 - accuracy: 0.8177\n",
            "Epoch 203/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3729 - accuracy: 0.8179\n",
            "Epoch 204/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3730 - accuracy: 0.8194\n",
            "Epoch 205/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3713 - accuracy: 0.8192\n",
            "Epoch 206/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3703 - accuracy: 0.8200\n",
            "Epoch 207/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3718 - accuracy: 0.8188\n",
            "Epoch 208/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3701 - accuracy: 0.8195\n",
            "Epoch 209/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3710 - accuracy: 0.8199\n",
            "Epoch 210/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3715 - accuracy: 0.8181\n",
            "Epoch 211/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3731 - accuracy: 0.8192\n",
            "Epoch 212/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3722 - accuracy: 0.8178\n",
            "Epoch 213/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3709 - accuracy: 0.8194\n",
            "Epoch 214/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3707 - accuracy: 0.8196\n",
            "Epoch 215/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3693 - accuracy: 0.8201\n",
            "Epoch 216/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3727 - accuracy: 0.8190\n",
            "Epoch 217/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.3705 - accuracy: 0.8201\n",
            "Epoch 218/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3684 - accuracy: 0.8207\n",
            "Epoch 219/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3706 - accuracy: 0.8202\n",
            "Epoch 220/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3699 - accuracy: 0.8199\n",
            "Epoch 221/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3689 - accuracy: 0.8204\n",
            "Epoch 222/250\n",
            "136/136 [==============================] - 0s 4ms/step - loss: 0.3693 - accuracy: 0.8213\n",
            "Epoch 223/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3700 - accuracy: 0.8208\n",
            "Epoch 224/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3677 - accuracy: 0.8212\n",
            "Epoch 225/250\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3702 - accuracy: 0.8192\n",
            "Epoch 226/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3692 - accuracy: 0.8215\n",
            "Epoch 227/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3682 - accuracy: 0.8207\n",
            "Epoch 228/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3668 - accuracy: 0.8207\n",
            "Epoch 229/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3698 - accuracy: 0.8202\n",
            "Epoch 230/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3709 - accuracy: 0.8187\n",
            "Epoch 231/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3707 - accuracy: 0.8199\n",
            "Epoch 232/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3680 - accuracy: 0.8195\n",
            "Epoch 233/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3705 - accuracy: 0.8197\n",
            "Epoch 234/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3696 - accuracy: 0.8195\n",
            "Epoch 235/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3688 - accuracy: 0.8222\n",
            "Epoch 236/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3703 - accuracy: 0.8188\n",
            "Epoch 237/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3682 - accuracy: 0.8203\n",
            "Epoch 238/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3685 - accuracy: 0.8196\n",
            "Epoch 239/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3686 - accuracy: 0.8209\n",
            "Epoch 240/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3692 - accuracy: 0.8200\n",
            "Epoch 241/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3679 - accuracy: 0.8202\n",
            "Epoch 242/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3683 - accuracy: 0.8205\n",
            "Epoch 243/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3665 - accuracy: 0.8227\n",
            "Epoch 244/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3700 - accuracy: 0.8210\n",
            "Epoch 245/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3693 - accuracy: 0.8207\n",
            "Epoch 246/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3668 - accuracy: 0.8212\n",
            "Epoch 247/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3693 - accuracy: 0.8209\n",
            "Epoch 248/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3679 - accuracy: 0.8203\n",
            "Epoch 249/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3699 - accuracy: 0.8199\n",
            "Epoch 250/250\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3686 - accuracy: 0.8207\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_27 (Dense)            (None, 32)                704       \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 32)               128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,033\n",
            "Trainable params: 19,777\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbYMFah4lb7n",
        "outputId": "734f829e-ff5f-4d69-b259-efe27563dbfd"
      },
      "source": [
        "\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "x_train = model.predict(train_X)\n",
        "knn.fit(x_train,train_Y)\n",
        "x_test = model.predict(test_X)\n",
        "y_pred = knn.predict(x_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "knn_accuracy = accuracy_score(test_Y, y_pred)\n",
        "print('Accuracy (RES + KNN): ', \"%.2f\" % (knn_accuracy*100))\n",
        "precision = precision_score(test_Y, y_pred)\n",
        "print('Precision: %f' % precision)\n",
        "\n",
        "recall = recall_score(test_Y, y_pred)\n",
        "print('Recall: %f' % recall)\n",
        "\n",
        "f1 = f1_score(test_Y, y_pred)\n",
        "print('F1 score: %f' % f1)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (RES + KNN):  81.20\n",
            "Precision: 0.545977\n",
            "Recall: 0.223529\n",
            "F1 score: 0.317195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWS1YFLVsRnc"
      },
      "source": [
        "Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "XhPBt7l6st0j",
        "outputId": "c90600ad-06b1-44a5-c0a6-8c28aa16a876"
      },
      "source": [
        "\n",
        "'''\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "fig = plt.pyplot.plot(figsize=(10,5))\n",
        "plt.pyplot.bar(test_Y,y_pred)\n",
        "\n",
        "plt.pyplot.plot(test_Y,test_Y,'r')\n",
        "'''"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f84bdf6fb10>]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH_4V2SByjfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c84ded0-0bf7-45a7-ed1a-00cfeb0fa4a4"
      },
      "source": [
        "'''from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "y_pred = model.predict(test_X)\n",
        "y_pred = (y_pred >= 0.6).astype(int).ravel()\n",
        " \n",
        "\n",
        "accuracy = accuracy_score(test_Y, y_pred)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "\n",
        "precision = precision_score(test_Y, y_pred)\n",
        "print('Precision: %f' % precision)\n",
        "\n",
        "recall = recall_score(test_Y, y_pred)\n",
        "print('Recall: %f' % recall)\n",
        "\n",
        "f1 = f1_score(test_Y, y_pred)\n",
        "print('F1 score: %f' % f1)\n",
        "'''\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.810662\n",
            "Precision: 0.622642\n",
            "Recall: 0.077647\n",
            "F1 score: 0.138075\n"
          ]
        }
      ]
    }
  ]
}