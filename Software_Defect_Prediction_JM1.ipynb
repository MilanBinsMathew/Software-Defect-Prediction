{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Software-Defect-Prediction",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONeea/+uTAPsrA9CpsBGyX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEMlVb_5biF0"
      },
      "source": [
        "Importing all nescessary packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWiqWojS2qJW"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import seaborn as sns\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdCeRHRAcFkb"
      },
      "source": [
        "Downloading & Printing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pSiCustcEzK"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/MilanBinsMathew/Software-Defect-Prediction/main/jm1_csv.csv'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFKX_vJCd1Bk",
        "outputId": "94bcd39d-c23a-4403-d2c1-0a929bf58dfc"
      },
      "source": [
        "df = pd.read_csv(url)\n",
        "print(df.head())\n",
        "\n",
        "df.isnull().sum()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     loc  v(g)  ev(g)  iv(g)  ...  total_Op  total_Opnd  branchCount  defects\n",
            "0    1.1   1.4    1.4    1.4  ...       1.2         1.2          1.4        0\n",
            "1    1.0   1.0    1.0    1.0  ...       1.0         1.0          1.0        1\n",
            "2   72.0   7.0    1.0    6.0  ...     112.0        86.0         13.0        1\n",
            "3  190.0   3.0    1.0    3.0  ...     329.0       271.0          5.0        1\n",
            "4   37.0   4.0    1.0    4.0  ...      76.0        50.0          7.0        1\n",
            "\n",
            "[5 rows x 22 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "loc                  0\n",
              "v(g)                 0\n",
              "ev(g)                0\n",
              "iv(g)                0\n",
              "n                    0\n",
              "v                    0\n",
              "l                    0\n",
              "d                    0\n",
              "i                    0\n",
              "e                    0\n",
              "b                    0\n",
              "t                    0\n",
              "lOCode               0\n",
              "lOComment            0\n",
              "lOBlank              0\n",
              "locCodeAndComment    0\n",
              "uniq_Op              0\n",
              "uniq_Opnd            0\n",
              "total_Op             0\n",
              "total_Opnd           0\n",
              "branchCount          0\n",
              "defects              0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2m3WgW3l7zp"
      },
      "source": [
        "Shuffling the dataset.\n",
        "Segementation into training and test set (80% - 20%)\n",
        "Normalizing the values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSG4zkugmM0Y"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop(['defects'], axis =1).values\n",
        "\n",
        "y = df['defects'].values\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3VMk2pJri8R"
      },
      "source": [
        "Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G-KTc-ArmWC"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=[0,1])\n",
        "X = scaler.fit_transform(X) \n",
        "\n",
        "train_X,test_X,train_Y,test_Y = train_test_split(X, y, test_size=0.2, random_state = 2)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2rWS3crnMva"
      },
      "source": [
        "\n",
        "\n",
        "Keras Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbso4gW0r_zM"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Activation,BatchNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TZgUlkbxblE"
      },
      "source": [
        "\n",
        "\n",
        "model = Sequential([ \n",
        "(Dense(32,activation = 'relu',input_shape = (21,))),\n",
        "(BatchNormalization()),\n",
        "(Dense(32,activation = 'relu')),\n",
        "(BatchNormalization()),\n",
        "(Dense(64,activation = 'relu')),\n",
        "(Dense(64,activation = 'relu')),\n",
        "(Dense(64,activation = 'relu')),\n",
        "(Dense(64,activation = 'relu')),\n",
        "(BatchNormalization()),\n",
        "(Dense(32,activation = 'relu')),\n",
        "(Dense(32,activation = 'relu')),\n",
        "(Dense(1,activation = 'linear',kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
        "])\n",
        "ad = Adam(learning_rate = 0.0001)\n",
        "\n",
        "\n",
        "model.compile(optimizer = ad, loss='hinge',metrics = ['accuracy'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGMY88g8rQuO"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88pIatPU85m-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c4e4b93-c6b2-41fa-e2df-91a67e869f6a"
      },
      "source": [
        "model.fit(x = train_X, y = train_Y,validation_data=(test_X,test_Y), batch_size = 64, epochs =200)\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "136/136 [==============================] - 2s 6ms/step - loss: 1.0413 - accuracy: 0.6767 - val_loss: 0.9749 - val_accuracy: 0.8047\n",
            "Epoch 2/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.5391 - accuracy: 0.8089 - val_loss: 0.7636 - val_accuracy: 0.8047\n",
            "Epoch 3/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4343 - accuracy: 0.8092 - val_loss: 0.5539 - val_accuracy: 0.8051\n",
            "Epoch 4/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4243 - accuracy: 0.8104 - val_loss: 0.4343 - val_accuracy: 0.8070\n",
            "Epoch 5/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4206 - accuracy: 0.8118 - val_loss: 0.4157 - val_accuracy: 0.8074\n",
            "Epoch 6/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.4120 - accuracy: 0.8128 - val_loss: 0.4182 - val_accuracy: 0.8088\n",
            "Epoch 7/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4138 - accuracy: 0.8114 - val_loss: 0.4165 - val_accuracy: 0.8088\n",
            "Epoch 8/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4080 - accuracy: 0.8136 - val_loss: 0.4163 - val_accuracy: 0.8116\n",
            "Epoch 9/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4052 - accuracy: 0.8158 - val_loss: 0.4135 - val_accuracy: 0.8130\n",
            "Epoch 10/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.4067 - accuracy: 0.8149 - val_loss: 0.4130 - val_accuracy: 0.8130\n",
            "Epoch 11/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.4072 - accuracy: 0.8145 - val_loss: 0.4094 - val_accuracy: 0.8130\n",
            "Epoch 12/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.4008 - accuracy: 0.8147 - val_loss: 0.4071 - val_accuracy: 0.8120\n",
            "Epoch 13/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3969 - accuracy: 0.8164 - val_loss: 0.4065 - val_accuracy: 0.8134\n",
            "Epoch 14/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3979 - accuracy: 0.8161 - val_loss: 0.4050 - val_accuracy: 0.8143\n",
            "Epoch 15/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3974 - accuracy: 0.8155 - val_loss: 0.4042 - val_accuracy: 0.8139\n",
            "Epoch 16/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3979 - accuracy: 0.8159 - val_loss: 0.4045 - val_accuracy: 0.8130\n",
            "Epoch 17/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3953 - accuracy: 0.8162 - val_loss: 0.4029 - val_accuracy: 0.8134\n",
            "Epoch 18/200\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3939 - accuracy: 0.8158 - val_loss: 0.4045 - val_accuracy: 0.8134\n",
            "Epoch 19/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3941 - accuracy: 0.8176 - val_loss: 0.4034 - val_accuracy: 0.8143\n",
            "Epoch 20/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3933 - accuracy: 0.8166 - val_loss: 0.4017 - val_accuracy: 0.8130\n",
            "Epoch 21/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3903 - accuracy: 0.8173 - val_loss: 0.4013 - val_accuracy: 0.8134\n",
            "Epoch 22/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3907 - accuracy: 0.8171 - val_loss: 0.3992 - val_accuracy: 0.8139\n",
            "Epoch 23/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3887 - accuracy: 0.8176 - val_loss: 0.3989 - val_accuracy: 0.8148\n",
            "Epoch 24/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3901 - accuracy: 0.8172 - val_loss: 0.4008 - val_accuracy: 0.8120\n",
            "Epoch 25/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3871 - accuracy: 0.8184 - val_loss: 0.3971 - val_accuracy: 0.8148\n",
            "Epoch 26/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3848 - accuracy: 0.8193 - val_loss: 0.3989 - val_accuracy: 0.8139\n",
            "Epoch 27/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3871 - accuracy: 0.8166 - val_loss: 0.3969 - val_accuracy: 0.8143\n",
            "Epoch 28/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3837 - accuracy: 0.8204 - val_loss: 0.3973 - val_accuracy: 0.8143\n",
            "Epoch 29/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3825 - accuracy: 0.8184 - val_loss: 0.3976 - val_accuracy: 0.8139\n",
            "Epoch 30/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3815 - accuracy: 0.8196 - val_loss: 0.3949 - val_accuracy: 0.8153\n",
            "Epoch 31/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3832 - accuracy: 0.8186 - val_loss: 0.3947 - val_accuracy: 0.8162\n",
            "Epoch 32/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3818 - accuracy: 0.8203 - val_loss: 0.3947 - val_accuracy: 0.8153\n",
            "Epoch 33/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3795 - accuracy: 0.8207 - val_loss: 0.3924 - val_accuracy: 0.8162\n",
            "Epoch 34/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3779 - accuracy: 0.8190 - val_loss: 0.3941 - val_accuracy: 0.8162\n",
            "Epoch 35/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3771 - accuracy: 0.8204 - val_loss: 0.3953 - val_accuracy: 0.8148\n",
            "Epoch 36/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3767 - accuracy: 0.8195 - val_loss: 0.3937 - val_accuracy: 0.8130\n",
            "Epoch 37/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3772 - accuracy: 0.8212 - val_loss: 0.3931 - val_accuracy: 0.8162\n",
            "Epoch 38/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3728 - accuracy: 0.8227 - val_loss: 0.3944 - val_accuracy: 0.8153\n",
            "Epoch 39/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3742 - accuracy: 0.8211 - val_loss: 0.3938 - val_accuracy: 0.8139\n",
            "Epoch 40/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3763 - accuracy: 0.8199 - val_loss: 0.3946 - val_accuracy: 0.8139\n",
            "Epoch 41/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3709 - accuracy: 0.8238 - val_loss: 0.3914 - val_accuracy: 0.8166\n",
            "Epoch 42/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3732 - accuracy: 0.8223 - val_loss: 0.3896 - val_accuracy: 0.8157\n",
            "Epoch 43/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3691 - accuracy: 0.8236 - val_loss: 0.3926 - val_accuracy: 0.8157\n",
            "Epoch 44/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3732 - accuracy: 0.8222 - val_loss: 0.3932 - val_accuracy: 0.8171\n",
            "Epoch 45/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3728 - accuracy: 0.8227 - val_loss: 0.3921 - val_accuracy: 0.8157\n",
            "Epoch 46/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3723 - accuracy: 0.8224 - val_loss: 0.3927 - val_accuracy: 0.8143\n",
            "Epoch 47/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3685 - accuracy: 0.8243 - val_loss: 0.3913 - val_accuracy: 0.8166\n",
            "Epoch 48/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3673 - accuracy: 0.8249 - val_loss: 0.3942 - val_accuracy: 0.8162\n",
            "Epoch 49/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3666 - accuracy: 0.8250 - val_loss: 0.3933 - val_accuracy: 0.8143\n",
            "Epoch 50/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3660 - accuracy: 0.8243 - val_loss: 0.3907 - val_accuracy: 0.8176\n",
            "Epoch 51/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3677 - accuracy: 0.8250 - val_loss: 0.3873 - val_accuracy: 0.8171\n",
            "Epoch 52/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3631 - accuracy: 0.8269 - val_loss: 0.3910 - val_accuracy: 0.8162\n",
            "Epoch 53/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3665 - accuracy: 0.8270 - val_loss: 0.3905 - val_accuracy: 0.8157\n",
            "Epoch 54/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3671 - accuracy: 0.8246 - val_loss: 0.3906 - val_accuracy: 0.8143\n",
            "Epoch 55/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3654 - accuracy: 0.8253 - val_loss: 0.3911 - val_accuracy: 0.8148\n",
            "Epoch 56/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3628 - accuracy: 0.8274 - val_loss: 0.3881 - val_accuracy: 0.8157\n",
            "Epoch 57/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3650 - accuracy: 0.8262 - val_loss: 0.3922 - val_accuracy: 0.8176\n",
            "Epoch 58/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3628 - accuracy: 0.8264 - val_loss: 0.3904 - val_accuracy: 0.8171\n",
            "Epoch 59/200\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3613 - accuracy: 0.8269 - val_loss: 0.3903 - val_accuracy: 0.8153\n",
            "Epoch 60/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3620 - accuracy: 0.8263 - val_loss: 0.3888 - val_accuracy: 0.8162\n",
            "Epoch 61/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3593 - accuracy: 0.8293 - val_loss: 0.3909 - val_accuracy: 0.8148\n",
            "Epoch 62/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3613 - accuracy: 0.8292 - val_loss: 0.3898 - val_accuracy: 0.8157\n",
            "Epoch 63/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3623 - accuracy: 0.8270 - val_loss: 0.3903 - val_accuracy: 0.8157\n",
            "Epoch 64/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3602 - accuracy: 0.8279 - val_loss: 0.3885 - val_accuracy: 0.8166\n",
            "Epoch 65/200\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3569 - accuracy: 0.8290 - val_loss: 0.3901 - val_accuracy: 0.8153\n",
            "Epoch 66/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3592 - accuracy: 0.8290 - val_loss: 0.3898 - val_accuracy: 0.8166\n",
            "Epoch 67/200\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3586 - accuracy: 0.8276 - val_loss: 0.3949 - val_accuracy: 0.8143\n",
            "Epoch 68/200\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3580 - accuracy: 0.8289 - val_loss: 0.3923 - val_accuracy: 0.8157\n",
            "Epoch 69/200\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3567 - accuracy: 0.8293 - val_loss: 0.3966 - val_accuracy: 0.8143\n",
            "Epoch 70/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3599 - accuracy: 0.8295 - val_loss: 0.3925 - val_accuracy: 0.8162\n",
            "Epoch 71/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3547 - accuracy: 0.8305 - val_loss: 0.3960 - val_accuracy: 0.8116\n",
            "Epoch 72/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3561 - accuracy: 0.8295 - val_loss: 0.3925 - val_accuracy: 0.8162\n",
            "Epoch 73/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3532 - accuracy: 0.8295 - val_loss: 0.3931 - val_accuracy: 0.8166\n",
            "Epoch 74/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3557 - accuracy: 0.8309 - val_loss: 0.3917 - val_accuracy: 0.8162\n",
            "Epoch 75/200\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3554 - accuracy: 0.8308 - val_loss: 0.3925 - val_accuracy: 0.8162\n",
            "Epoch 76/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3506 - accuracy: 0.8328 - val_loss: 0.3901 - val_accuracy: 0.8148\n",
            "Epoch 77/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3557 - accuracy: 0.8288 - val_loss: 0.3932 - val_accuracy: 0.8162\n",
            "Epoch 78/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3524 - accuracy: 0.8315 - val_loss: 0.3956 - val_accuracy: 0.8143\n",
            "Epoch 79/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3548 - accuracy: 0.8298 - val_loss: 0.3946 - val_accuracy: 0.8148\n",
            "Epoch 80/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3554 - accuracy: 0.8279 - val_loss: 0.3939 - val_accuracy: 0.8148\n",
            "Epoch 81/200\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3525 - accuracy: 0.8311 - val_loss: 0.3915 - val_accuracy: 0.8157\n",
            "Epoch 82/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3510 - accuracy: 0.8315 - val_loss: 0.3953 - val_accuracy: 0.8157\n",
            "Epoch 83/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3508 - accuracy: 0.8312 - val_loss: 0.3931 - val_accuracy: 0.8166\n",
            "Epoch 84/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3516 - accuracy: 0.8320 - val_loss: 0.3922 - val_accuracy: 0.8153\n",
            "Epoch 85/200\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3513 - accuracy: 0.8320 - val_loss: 0.3955 - val_accuracy: 0.8130\n",
            "Epoch 86/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3466 - accuracy: 0.8339 - val_loss: 0.3937 - val_accuracy: 0.8143\n",
            "Epoch 87/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3506 - accuracy: 0.8333 - val_loss: 0.3950 - val_accuracy: 0.8134\n",
            "Epoch 88/200\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3476 - accuracy: 0.8327 - val_loss: 0.3958 - val_accuracy: 0.8139\n",
            "Epoch 89/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3489 - accuracy: 0.8323 - val_loss: 0.3978 - val_accuracy: 0.8143\n",
            "Epoch 90/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3486 - accuracy: 0.8334 - val_loss: 0.3986 - val_accuracy: 0.8134\n",
            "Epoch 91/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3478 - accuracy: 0.8326 - val_loss: 0.3943 - val_accuracy: 0.8148\n",
            "Epoch 92/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3451 - accuracy: 0.8341 - val_loss: 0.3977 - val_accuracy: 0.8130\n",
            "Epoch 93/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3463 - accuracy: 0.8330 - val_loss: 0.3956 - val_accuracy: 0.8134\n",
            "Epoch 94/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3464 - accuracy: 0.8340 - val_loss: 0.3926 - val_accuracy: 0.8143\n",
            "Epoch 95/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3464 - accuracy: 0.8338 - val_loss: 0.3920 - val_accuracy: 0.8134\n",
            "Epoch 96/200\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3456 - accuracy: 0.8340 - val_loss: 0.3965 - val_accuracy: 0.8153\n",
            "Epoch 97/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3433 - accuracy: 0.8351 - val_loss: 0.3958 - val_accuracy: 0.8143\n",
            "Epoch 98/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3456 - accuracy: 0.8347 - val_loss: 0.3941 - val_accuracy: 0.8148\n",
            "Epoch 99/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3447 - accuracy: 0.8336 - val_loss: 0.3922 - val_accuracy: 0.8176\n",
            "Epoch 100/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3443 - accuracy: 0.8348 - val_loss: 0.3919 - val_accuracy: 0.8134\n",
            "Epoch 101/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3464 - accuracy: 0.8340 - val_loss: 0.3949 - val_accuracy: 0.8143\n",
            "Epoch 102/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3426 - accuracy: 0.8335 - val_loss: 0.3934 - val_accuracy: 0.8157\n",
            "Epoch 103/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3424 - accuracy: 0.8365 - val_loss: 0.3945 - val_accuracy: 0.8153\n",
            "Epoch 104/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3435 - accuracy: 0.8354 - val_loss: 0.3917 - val_accuracy: 0.8148\n",
            "Epoch 105/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3422 - accuracy: 0.8338 - val_loss: 0.3924 - val_accuracy: 0.8153\n",
            "Epoch 106/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3418 - accuracy: 0.8364 - val_loss: 0.3909 - val_accuracy: 0.8134\n",
            "Epoch 107/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3413 - accuracy: 0.8357 - val_loss: 0.3931 - val_accuracy: 0.8134\n",
            "Epoch 108/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3393 - accuracy: 0.8371 - val_loss: 0.3930 - val_accuracy: 0.8162\n",
            "Epoch 109/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3408 - accuracy: 0.8352 - val_loss: 0.3925 - val_accuracy: 0.8148\n",
            "Epoch 110/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3402 - accuracy: 0.8375 - val_loss: 0.3967 - val_accuracy: 0.8116\n",
            "Epoch 111/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3420 - accuracy: 0.8356 - val_loss: 0.3988 - val_accuracy: 0.8139\n",
            "Epoch 112/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3388 - accuracy: 0.8372 - val_loss: 0.3971 - val_accuracy: 0.8139\n",
            "Epoch 113/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3369 - accuracy: 0.8365 - val_loss: 0.3984 - val_accuracy: 0.8143\n",
            "Epoch 114/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3382 - accuracy: 0.8378 - val_loss: 0.3964 - val_accuracy: 0.8120\n",
            "Epoch 115/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3391 - accuracy: 0.8369 - val_loss: 0.3941 - val_accuracy: 0.8134\n",
            "Epoch 116/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3382 - accuracy: 0.8378 - val_loss: 0.3948 - val_accuracy: 0.8157\n",
            "Epoch 117/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3359 - accuracy: 0.8372 - val_loss: 0.3996 - val_accuracy: 0.8134\n",
            "Epoch 118/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3371 - accuracy: 0.8378 - val_loss: 0.3958 - val_accuracy: 0.8148\n",
            "Epoch 119/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3345 - accuracy: 0.8397 - val_loss: 0.3999 - val_accuracy: 0.8143\n",
            "Epoch 120/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3377 - accuracy: 0.8367 - val_loss: 0.3991 - val_accuracy: 0.8134\n",
            "Epoch 121/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3357 - accuracy: 0.8371 - val_loss: 0.3949 - val_accuracy: 0.8148\n",
            "Epoch 122/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3349 - accuracy: 0.8392 - val_loss: 0.3960 - val_accuracy: 0.8166\n",
            "Epoch 123/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3337 - accuracy: 0.8393 - val_loss: 0.3993 - val_accuracy: 0.8143\n",
            "Epoch 124/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3397 - accuracy: 0.8369 - val_loss: 0.3973 - val_accuracy: 0.8162\n",
            "Epoch 125/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3359 - accuracy: 0.8377 - val_loss: 0.3947 - val_accuracy: 0.8120\n",
            "Epoch 126/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3353 - accuracy: 0.8364 - val_loss: 0.4012 - val_accuracy: 0.8120\n",
            "Epoch 127/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3357 - accuracy: 0.8382 - val_loss: 0.4012 - val_accuracy: 0.8130\n",
            "Epoch 128/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3351 - accuracy: 0.8382 - val_loss: 0.3981 - val_accuracy: 0.8134\n",
            "Epoch 129/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3347 - accuracy: 0.8384 - val_loss: 0.3963 - val_accuracy: 0.8162\n",
            "Epoch 130/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3367 - accuracy: 0.8375 - val_loss: 0.3961 - val_accuracy: 0.8180\n",
            "Epoch 131/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3326 - accuracy: 0.8387 - val_loss: 0.3960 - val_accuracy: 0.8125\n",
            "Epoch 132/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3311 - accuracy: 0.8402 - val_loss: 0.4007 - val_accuracy: 0.8116\n",
            "Epoch 133/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3342 - accuracy: 0.8388 - val_loss: 0.4049 - val_accuracy: 0.8116\n",
            "Epoch 134/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8419 - val_loss: 0.4026 - val_accuracy: 0.8097\n",
            "Epoch 135/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3309 - accuracy: 0.8409 - val_loss: 0.4032 - val_accuracy: 0.8111\n",
            "Epoch 136/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3330 - accuracy: 0.8388 - val_loss: 0.4078 - val_accuracy: 0.8143\n",
            "Epoch 137/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8417 - val_loss: 0.4004 - val_accuracy: 0.8130\n",
            "Epoch 138/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3286 - accuracy: 0.8415 - val_loss: 0.4043 - val_accuracy: 0.8097\n",
            "Epoch 139/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3312 - accuracy: 0.8404 - val_loss: 0.4015 - val_accuracy: 0.8157\n",
            "Epoch 140/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3309 - accuracy: 0.8392 - val_loss: 0.4058 - val_accuracy: 0.8134\n",
            "Epoch 141/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3301 - accuracy: 0.8411 - val_loss: 0.4015 - val_accuracy: 0.8143\n",
            "Epoch 142/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3305 - accuracy: 0.8400 - val_loss: 0.4034 - val_accuracy: 0.8102\n",
            "Epoch 143/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8426 - val_loss: 0.4035 - val_accuracy: 0.8097\n",
            "Epoch 144/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3267 - accuracy: 0.8416 - val_loss: 0.4012 - val_accuracy: 0.8107\n",
            "Epoch 145/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8413 - val_loss: 0.4048 - val_accuracy: 0.8111\n",
            "Epoch 146/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8412 - val_loss: 0.4097 - val_accuracy: 0.8130\n",
            "Epoch 147/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3273 - accuracy: 0.8410 - val_loss: 0.4054 - val_accuracy: 0.8116\n",
            "Epoch 148/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3244 - accuracy: 0.8436 - val_loss: 0.4064 - val_accuracy: 0.8120\n",
            "Epoch 149/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3287 - accuracy: 0.8411 - val_loss: 0.4044 - val_accuracy: 0.8148\n",
            "Epoch 150/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8418 - val_loss: 0.4055 - val_accuracy: 0.8097\n",
            "Epoch 151/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8427 - val_loss: 0.4085 - val_accuracy: 0.8116\n",
            "Epoch 152/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3238 - accuracy: 0.8442 - val_loss: 0.4059 - val_accuracy: 0.8102\n",
            "Epoch 153/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3231 - accuracy: 0.8449 - val_loss: 0.4073 - val_accuracy: 0.8125\n",
            "Epoch 154/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3252 - accuracy: 0.8434 - val_loss: 0.4103 - val_accuracy: 0.8157\n",
            "Epoch 155/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3247 - accuracy: 0.8436 - val_loss: 0.4134 - val_accuracy: 0.8102\n",
            "Epoch 156/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3254 - accuracy: 0.8424 - val_loss: 0.4070 - val_accuracy: 0.8097\n",
            "Epoch 157/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8433 - val_loss: 0.4100 - val_accuracy: 0.8107\n",
            "Epoch 158/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3237 - accuracy: 0.8440 - val_loss: 0.4128 - val_accuracy: 0.8111\n",
            "Epoch 159/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3222 - accuracy: 0.8447 - val_loss: 0.4104 - val_accuracy: 0.8107\n",
            "Epoch 160/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.8446 - val_loss: 0.4058 - val_accuracy: 0.8107\n",
            "Epoch 161/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8433 - val_loss: 0.4070 - val_accuracy: 0.8130\n",
            "Epoch 162/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3231 - accuracy: 0.8436 - val_loss: 0.4102 - val_accuracy: 0.8097\n",
            "Epoch 163/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3216 - accuracy: 0.8448 - val_loss: 0.4103 - val_accuracy: 0.8157\n",
            "Epoch 164/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3242 - accuracy: 0.8433 - val_loss: 0.4090 - val_accuracy: 0.8102\n",
            "Epoch 165/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3221 - accuracy: 0.8433 - val_loss: 0.4104 - val_accuracy: 0.8134\n",
            "Epoch 166/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3206 - accuracy: 0.8458 - val_loss: 0.4120 - val_accuracy: 0.8107\n",
            "Epoch 167/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3187 - accuracy: 0.8463 - val_loss: 0.4177 - val_accuracy: 0.8088\n",
            "Epoch 168/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.8431 - val_loss: 0.4150 - val_accuracy: 0.8116\n",
            "Epoch 169/200\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3214 - accuracy: 0.8448 - val_loss: 0.4082 - val_accuracy: 0.8116\n",
            "Epoch 170/200\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3182 - accuracy: 0.8454 - val_loss: 0.4081 - val_accuracy: 0.8102\n",
            "Epoch 171/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3226 - accuracy: 0.8440 - val_loss: 0.4107 - val_accuracy: 0.8116\n",
            "Epoch 172/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3197 - accuracy: 0.8456 - val_loss: 0.4101 - val_accuracy: 0.8130\n",
            "Epoch 173/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3218 - accuracy: 0.8442 - val_loss: 0.4171 - val_accuracy: 0.8093\n",
            "Epoch 174/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3196 - accuracy: 0.8462 - val_loss: 0.4124 - val_accuracy: 0.8102\n",
            "Epoch 175/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3183 - accuracy: 0.8443 - val_loss: 0.4143 - val_accuracy: 0.8111\n",
            "Epoch 176/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3167 - accuracy: 0.8457 - val_loss: 0.4150 - val_accuracy: 0.8111\n",
            "Epoch 177/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3175 - accuracy: 0.8472 - val_loss: 0.4139 - val_accuracy: 0.8157\n",
            "Epoch 178/200\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3184 - accuracy: 0.8460 - val_loss: 0.4140 - val_accuracy: 0.8120\n",
            "Epoch 179/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3156 - accuracy: 0.8465 - val_loss: 0.4086 - val_accuracy: 0.8134\n",
            "Epoch 180/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3161 - accuracy: 0.8471 - val_loss: 0.4070 - val_accuracy: 0.8116\n",
            "Epoch 181/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8465 - val_loss: 0.4077 - val_accuracy: 0.8111\n",
            "Epoch 182/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3172 - accuracy: 0.8472 - val_loss: 0.4110 - val_accuracy: 0.8162\n",
            "Epoch 183/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3157 - accuracy: 0.8483 - val_loss: 0.4079 - val_accuracy: 0.8125\n",
            "Epoch 184/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3192 - accuracy: 0.8460 - val_loss: 0.4137 - val_accuracy: 0.8111\n",
            "Epoch 185/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3158 - accuracy: 0.8465 - val_loss: 0.4127 - val_accuracy: 0.8111\n",
            "Epoch 186/200\n",
            "136/136 [==============================] - 1s 4ms/step - loss: 0.3161 - accuracy: 0.8470 - val_loss: 0.4141 - val_accuracy: 0.8074\n",
            "Epoch 187/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3171 - accuracy: 0.8473 - val_loss: 0.4140 - val_accuracy: 0.8116\n",
            "Epoch 188/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3179 - accuracy: 0.8454 - val_loss: 0.4141 - val_accuracy: 0.8093\n",
            "Epoch 189/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3214 - accuracy: 0.8443 - val_loss: 0.4277 - val_accuracy: 0.8143\n",
            "Epoch 190/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3202 - accuracy: 0.8456 - val_loss: 0.4113 - val_accuracy: 0.8120\n",
            "Epoch 191/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3160 - accuracy: 0.8466 - val_loss: 0.4126 - val_accuracy: 0.8130\n",
            "Epoch 192/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3119 - accuracy: 0.8491 - val_loss: 0.4146 - val_accuracy: 0.8097\n",
            "Epoch 193/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3121 - accuracy: 0.8504 - val_loss: 0.4151 - val_accuracy: 0.8130\n",
            "Epoch 194/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3121 - accuracy: 0.8487 - val_loss: 0.4156 - val_accuracy: 0.8107\n",
            "Epoch 195/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3098 - accuracy: 0.8508 - val_loss: 0.4150 - val_accuracy: 0.8093\n",
            "Epoch 196/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3133 - accuracy: 0.8473 - val_loss: 0.4130 - val_accuracy: 0.8120\n",
            "Epoch 197/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3169 - accuracy: 0.8472 - val_loss: 0.4174 - val_accuracy: 0.8107\n",
            "Epoch 198/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3117 - accuracy: 0.8482 - val_loss: 0.4163 - val_accuracy: 0.8093\n",
            "Epoch 199/200\n",
            "136/136 [==============================] - 1s 5ms/step - loss: 0.3135 - accuracy: 0.8489 - val_loss: 0.4267 - val_accuracy: 0.8084\n",
            "Epoch 200/200\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3126 - accuracy: 0.8481 - val_loss: 0.4212 - val_accuracy: 0.8120\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 32)                704       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 32)               128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 32)               128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,033\n",
            "Trainable params: 19,777\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbYMFah4lb7n",
        "outputId": "65542612-0d5d-49dd-ecbe-a96d619577ef"
      },
      "source": [
        "y_pred = model.predict(test_X)\n",
        "y_pred = (y_pred >= 0.65).astype(int).ravel()\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "svm_accuracy = accuracy_score(test_Y, y_pred)\n",
        "print('Accuracy (ANN +SVM): ', \"%.2f\" % (svm_accuracy*100))\n",
        "precision = precision_score(test_Y, y_pred)\n",
        "print('Precision: %f' % precision)\n",
        "\n",
        "recall = recall_score(test_Y, y_pred)\n",
        "print('Recall: %f' % recall)\n",
        "\n",
        "f1 = f1_score(test_Y, y_pred)\n",
        "print('F1 score: %f' % f1)\n",
        "\n",
        "print('R2 Score:', roc_auc_score(test_Y,y_pred))\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (ANN +SVM):  81.25\n",
            "Precision: 0.572650\n",
            "Recall: 0.157647\n",
            "F1 score: 0.247232\n",
            "R2 Score: 0.5645459737292976\n"
          ]
        }
      ]
    }
  ]
}